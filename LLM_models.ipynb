{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wr8-qW52JRF7Qx0X2e3LArORTvbRiRoX",
      "authorship_tag": "ABX9TyPf5mcLuK9C6xUq16Bor4A8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushi-Rastogii/LLM-models/blob/main/LLM_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AutoChip**"
      ],
      "metadata": {
        "id": "cSyXeDhK1KMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/shailja-thakur/AutoChip.git\n",
        "!ls\n",
        "!cd AutoChip\n",
        "!python3 --version"
      ],
      "metadata": {
        "id": "7wlG4-VLlcr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r ./AutoChip/requirements.txt"
      ],
      "metadata": {
        "id": "GVMt7LqM0xjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido python-multipart\n",
        "!pip install langchain openai==0.28 cohere chromadb tiktoken unstructured"
      ],
      "metadata": {
        "id": "ri_BCEZDFnRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OPEN_API_KEY=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\n",
        "#PALM_API_KEY=AIzaSyDldCg9i8Fl6nUWEZopFUXS7qyb5Kxnwnc"
      ],
      "metadata": {
        "id": "Jsngv6brFrly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY=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\n",
        "%env PALM_API_KEY=AIzaSyDldCg9i8Fl6nUWEZopFUXS7qyb5Kxnwnc"
      ],
      "metadata": {
        "id": "OKmJOnJsHZDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run AutoChip/autochip_scripts/auto_create_verilog.py --prompt=AutoChip/hdlbits_prompts/7420.v --name=\"top_module\" --testbench=AutoChip/hdlbits_testbenches/7420_0_tb.v --iter=1 --model=PaLM --log=/content/run.v"
      ],
      "metadata": {
        "id": "NvLymJahmW51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain**"
      ],
      "metadata": {
        "id": "5Rsbyi5J1-Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaleido python-multipart\n",
        "!pip install langchain openai cohere chromadb tiktoken unstructured uvicorn fastapi"
      ],
      "metadata": {
        "id": "20OUGwoY2Bkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenAI**"
      ],
      "metadata": {
        "id": "19Gc5HbJ1yB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import openai\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "#import constants\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Enable to save to disk & reuse the model (for repeated queries on the same data)\n",
        "PERSIST = False\n",
        "\n",
        "query = \"what is spectre\"\n",
        "if len(sys.argv) > 1:\n",
        "  query = sys.argv[1]\n",
        "\n",
        "if PERSIST and os.path.exists(\"persist\"):\n",
        "  print(\"Reusing index...\\n\")\n",
        "  vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\n",
        "  index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
        "else:\n",
        "  loader = TextLoader(\"drive/MyDrive/MicroLLM/Data/Me.txt\") # Use this line if you only need data.txt\n",
        "  #loader = UnstructuredPDFLoader(\"./drive/MyDrive/MicroLLM/Data/Online_Detection_of_Spectre_Attacks_Using_Microarchitectural_Traces_from_Performance_Counters-1-2-1.pdf\")\n",
        "  #loader = DirectoryLoader(\"./drive/MyDrive/MicroLLM/Data/\")\n",
        "  if PERSIST:\n",
        "    index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":\"persist\"}).from_loaders([loader])\n",
        "  else:\n",
        "    index = VectorstoreIndexCreator().from_loaders([loader])\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "  llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
        "  retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "  if not query:\n",
        "    query = input(\"Prompt: \")\n",
        "  if query in ['quit', 'q', 'exit']:\n",
        "    sys.exit()\n",
        "  result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "  print(result['answer'])\n",
        "\n",
        "  chat_history.append((query, result['answer']))\n",
        "  query = None"
      ],
      "metadata": {
        "id": "jiXYR9u711RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PaLM**"
      ],
      "metadata": {
        "id": "3dWkvQa61RAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env Google_API_KEY=AIzaSyDldCg9i8Fl6nUWEZopFUXS7qyb5Kxnwnc"
      ],
      "metadata": {
        "id": "Cekwn3uKG4_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ti8_sn1_HN-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import google.generativeai as palm\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
        "from langchain.chat_models import ChatGooglePalm\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import GooglePalm\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "#import constants\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDldCg9i8Fl6nUWEZopFUXS7qyb5Kxnwnc\"\n",
        "\n",
        "# Enable to save to disk & reuse the model (for repeated queries on the same data)\n",
        "PERSIST = False\n",
        "\n",
        "query = \"what is spectre\"\n",
        "if len(sys.argv) > 1:\n",
        "  query = sys.argv[1]\n",
        "\n",
        "if PERSIST and os.path.exists(\"persist\"):\n",
        "  print(\"Reusing index...\\n\")\n",
        "  vectorstore = Chroma(embeddings=GooglePalmEmbeddings,persist_directory=\"persist\")\n",
        "  index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
        "else:\n",
        "  loader = TextLoader(\"drive/MyDrive/MicroLLM/abstract.txt\") # Use this line if you only need data.txt\n",
        "  #loader = UnstructuredPDFLoader(\"./drive/MyDrive/MicroLLM/Data/Online_Detection_of_Spectre_Attacks_Using_Microarchitectural_Traces_from_Performance_Counters-1-2-1.pdf\")\n",
        "  #loader = DirectoryLoader(\"./drive/MyDrive/MicroLLM/Data/\")\n",
        "  if PERSIST:\n",
        "    index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":\"persist\"}).from_loaders([loader])\n",
        "  else:\n",
        "    index = VectorstoreIndexCreator(embedding=GooglePalmEmbeddings()).from_loaders([loader])\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "  llm=ChatGooglePalm(model=\"GooglePaLM\"),\n",
        "  retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chat_history = []\n",
        "while True:\n",
        "  if not query:\n",
        "    query = input(\"Prompt: \")\n",
        "  if query in ['quit', 'q', 'exit']:\n",
        "    sys.exit()\n",
        "  result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "  print(result['answer'])\n",
        "\n",
        "  chat_history.append((query, result['answer']))\n",
        "  query = None"
      ],
      "metadata": {
        "id": "Nhsp1N6v1qu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7pIqvVNLG09N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}